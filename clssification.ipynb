{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "aa0a2518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "82a49db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training segments: 140\n",
      "Number of validation segments: 35\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('./Datasets/master_csv_bicep_0.csv')\n",
    "\n",
    "# Choose features\n",
    "imu_features = [\n",
    "    'accel_y', 'accel_x', 'accel_z',\n",
    "    'gyro_z', 'gyro_y', 'gyro_x',\n",
    "]\n",
    "target_column = 'part'\n",
    "\n",
    "# Use segment UIDs to split at the segment level\n",
    "segment_uids = df['Segment UID'].unique()\n",
    "\n",
    "# 80-20 split\n",
    "train_uids, val_uids = train_test_split(segment_uids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare training data\n",
    "train_imu_segments_bicep = []\n",
    "train_emg_segments_bicep = []\n",
    "\n",
    "for segment_uid in train_uids:\n",
    "    segment_df = df[df['Segment UID'] == segment_uid]\n",
    "    imu_data = segment_df[imu_features].values\n",
    "    emg_data = segment_df[target_column].values\n",
    "    train_imu_segments_bicep.append(imu_data)\n",
    "    train_emg_segments_bicep.append(emg_data)\n",
    "\n",
    "# Prepare validation data\n",
    "val_imu_segments_bicep = []\n",
    "val_emg_segments_bicep = []\n",
    "\n",
    "for segment_uid in val_uids:\n",
    "    segment_df = df[df['Segment UID'] == segment_uid]\n",
    "    imu_data = segment_df[imu_features].values\n",
    "    emg_data = segment_df[target_column].values\n",
    "    val_imu_segments_bicep.append(imu_data)\n",
    "    val_emg_segments_bicep.append(emg_data)\n",
    "\n",
    "print(f\"Number of training segments: {len(train_imu_segments_bicep)}\")\n",
    "print(f\"Number of validation segments: {len(val_imu_segments_bicep)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "75a5d5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training segments: 140\n",
      "Number of validation segments: 35\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('./Datasets/master_csv_tricep_0.csv')\n",
    "\n",
    "# Choose features\n",
    "imu_features = [\n",
    "    'accel_y', 'accel_x', 'accel_z',\n",
    "    'gyro_z', 'gyro_y', 'gyro_x',\n",
    "]\n",
    "target_column = 'part'\n",
    "\n",
    "# Use segment UIDs to split at the segment level\n",
    "segment_uids = df['Segment UID'].unique()\n",
    "\n",
    "# 80-20 split\n",
    "train_uids, val_uids = train_test_split(segment_uids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare training data\n",
    "train_imu_segments_tricep = []\n",
    "train_emg_segments_tricep = []\n",
    "\n",
    "for segment_uid in train_uids:\n",
    "    segment_df = df[df['Segment UID'] == segment_uid]\n",
    "    imu_data = segment_df[imu_features].values\n",
    "    emg_data = segment_df[target_column].values\n",
    "    train_imu_segments_tricep.append(imu_data)\n",
    "    train_emg_segments_tricep.append(emg_data)\n",
    "\n",
    "# Prepare validation data\n",
    "val_imu_segments_tricep = []\n",
    "val_emg_segments_tricep = []\n",
    "\n",
    "for segment_uid in val_uids:\n",
    "    segment_df = df[df['Segment UID'] == segment_uid]\n",
    "    imu_data = segment_df[imu_features].values\n",
    "    emg_data = segment_df[target_column].values\n",
    "    val_imu_segments_tricep.append(imu_data)\n",
    "    val_emg_segments_tricep.append(emg_data)\n",
    "\n",
    "print(f\"Number of training segments: {len(train_imu_segments_tricep)}\")\n",
    "print(f\"Number of validation segments: {len(val_imu_segments_tricep)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "09a88dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training segments: 140\n",
      "Number of validation segments: 35\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('./Datasets/master_csv_Supination_1.csv')\n",
    "\n",
    "# Choose features\n",
    "imu_features = [\n",
    "    'accel_y', 'accel_x', 'accel_z',\n",
    "    'gyro_z', 'gyro_y', 'gyro_x',\n",
    "]\n",
    "target_column = 'part'\n",
    "\n",
    "# Use segment UIDs to split at the segment level\n",
    "segment_uids = df['Segment UID'].unique()\n",
    "\n",
    "# 80-20 split\n",
    "train_uids, val_uids = train_test_split(segment_uids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare training data\n",
    "train_imu_segments_supination = []\n",
    "train_emg_segments_supination = []\n",
    "\n",
    "for segment_uid in train_uids:\n",
    "    segment_df = df[df['Segment UID'] == segment_uid]\n",
    "    imu_data = segment_df[imu_features].values\n",
    "    emg_data = segment_df[target_column].values\n",
    "    train_imu_segments_supination.append(imu_data)\n",
    "    train_emg_segments_supination.append(emg_data)\n",
    "\n",
    "# Prepare validation data\n",
    "val_imu_segments_supination = []\n",
    "val_emg_segments_supination = []\n",
    "\n",
    "for segment_uid in val_uids:\n",
    "    segment_df = df[df['Segment UID'] == segment_uid]\n",
    "    imu_data = segment_df[imu_features].values\n",
    "    emg_data = segment_df[target_column].values\n",
    "    val_imu_segments_supination.append(imu_data)\n",
    "    val_emg_segments_supination.append(emg_data)\n",
    "\n",
    "print(f\"Number of training segments: {len(train_imu_segments_supination)}\")\n",
    "print(f\"Number of validation segments: {len(val_imu_segments_supination)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cd1be874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training segments: 140\n",
      "Number of validation segments: 35\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('./Datasets/master_csv_Pronation_1.csv')\n",
    "\n",
    "# Choose features\n",
    "imu_features = [\n",
    "    'accel_y', 'accel_x', 'accel_z',\n",
    "    'gyro_z', 'gyro_y', 'gyro_x',\n",
    "]\n",
    "target_column = 'part'\n",
    "\n",
    "# Use segment UIDs to split at the segment level\n",
    "segment_uids = df['Segment UID'].unique()\n",
    "\n",
    "# 80-20 split\n",
    "train_uids, val_uids = train_test_split(segment_uids, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare training data\n",
    "train_imu_segments_pronation = []\n",
    "train_emg_segments_pronation = []\n",
    "\n",
    "for segment_uid in train_uids:\n",
    "    segment_df = df[df['Segment UID'] == segment_uid]\n",
    "    imu_data = segment_df[imu_features].values\n",
    "    emg_data = segment_df[target_column].values\n",
    "    train_imu_segments_pronation.append(imu_data)\n",
    "    train_emg_segments_pronation.append(emg_data)\n",
    "\n",
    "# Prepare validation data\n",
    "val_imu_segments_pronation = []\n",
    "val_emg_segments_pronation = []\n",
    "\n",
    "for segment_uid in val_uids:\n",
    "    segment_df = df[df['Segment UID'] == segment_uid]\n",
    "    imu_data = segment_df[imu_features].values\n",
    "    emg_data = segment_df[target_column].values\n",
    "    val_imu_segments_pronation.append(imu_data)\n",
    "    val_emg_segments_pronation.append(emg_data)\n",
    "\n",
    "print(f\"Number of training segments: {len(train_imu_segments_pronation)}\")\n",
    "print(f\"Number of validation segments: {len(val_imu_segments_pronation)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a6013ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMU_EMG_Dataset(Dataset):\n",
    "    def __init__(self, imu_segments, emg_segments):\n",
    "        self.imu_segments = imu_segments\n",
    "        self.emg_segments = emg_segments\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imu_segments)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imu = torch.tensor(self.imu_segments[idx], dtype=torch.float32)   # shape: (seq_len, imu_features)\n",
    "        emg = torch.tensor(self.emg_segments[idx], dtype=torch.float32)   # shape: (seq_len,)\n",
    "        emg = emg.unsqueeze(-1)  # Make it (seq_len, 1) to match output shape\n",
    "        return imu, emg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ee6e0341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    imu_batch, emg_batch = zip(*batch)\n",
    "    \n",
    "    # Pad sequences to the max length in this batch\n",
    "    imu_batch_padded = pad_sequence(imu_batch, batch_first=True)     # (batch_size, max_seq_len, imu_features)\n",
    "    emg_batch_padded = pad_sequence(emg_batch, batch_first=True)     # (batch_size, max_seq_len, 1)\n",
    "\n",
    "    # Create padding mask: shape (batch_size, max_seq_len)\n",
    "    # False where there's real data, True where there's padding\n",
    "    lengths = torch.tensor([x.shape[0] for x in imu_batch])\n",
    "    max_len = imu_batch_padded.shape[1]\n",
    "    pad_mask = torch.arange(max_len).expand(len(lengths), max_len) >= lengths.unsqueeze(1)\n",
    "    \n",
    "    return imu_batch_padded, emg_batch_padded, pad_mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "219d0916",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imu_segments = train_imu_segments_bicep + train_imu_segments_tricep + train_imu_segments_supination + train_imu_segments_pronation\n",
    "train_emg_segments = train_emg_segments_bicep + train_emg_segments_tricep + train_emg_segments_supination + train_emg_segments_pronation\n",
    "\n",
    "val_imu_segments = val_imu_segments_bicep + val_imu_segments_tricep + val_imu_segments_supination + val_imu_segments_pronation\n",
    "val_emg_segments = val_emg_segments_bicep + val_emg_segments_tricep + val_emg_segments_supination + val_emg_segments_pronation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e3f91630",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMU_EMG_Dataset(train_imu_segments, train_emg_segments)\n",
    "val_dataset = IMU_EMG_Dataset(val_imu_segments, val_emg_segments)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8abee722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated classification model and dataset for bicep/tricep classification\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import numpy as np\n",
    "\n",
    "class IMU_Classification_Dataset(Dataset):\n",
    "    def __init__(self, imu_segments, labels):\n",
    "        \"\"\"\n",
    "        Dataset for IMU sequence classification\n",
    "        Args:\n",
    "            imu_segments: List of IMU data segments (each segment is numpy array of shape [seq_len, features])\n",
    "            labels: List of labels (0 for bicep, 1 for tricep, 2 for supination, 3 for pronation)\n",
    "        \"\"\"\n",
    "        self.imu_segments = imu_segments\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imu_segments)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imu = torch.tensor(self.imu_segments[idx], dtype=torch.float32)\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return imu, label\n",
    "\n",
    "def classification_collate_fn(batch):\n",
    "    \"\"\"Collate function for classification dataset\"\"\"\n",
    "    imu_batch, labels_batch = zip(*batch)\n",
    "    \n",
    "    # Pad sequences to the max length in this batch\n",
    "    imu_batch_padded = pad_sequence(imu_batch, batch_first=True)\n",
    "    labels_batch = torch.stack(labels_batch)\n",
    "    \n",
    "    # Create padding mask\n",
    "    lengths = torch.tensor([x.shape[0] for x in imu_batch])\n",
    "    max_len = imu_batch_padded.shape[1]\n",
    "    pad_mask = torch.arange(max_len).expand(len(lengths), max_len) >= lengths.unsqueeze(1)\n",
    "    \n",
    "    return imu_batch_padded, labels_batch, pad_mask\n",
    "\n",
    "class IMU_Classifier(nn.Module):\n",
    "    def __init__(self, input_size=6, hidden_size=128, num_layers=2, num_classes=4, dropout=0.3):\n",
    "        \"\"\"\n",
    "        LSTM-based classifier for IMU sequences\n",
    "        Args:\n",
    "            input_size: Number of IMU features (6: accel_x,y,z + gyro_x,y,z)\n",
    "            hidden_size: Hidden size of LSTM\n",
    "            num_layers: Number of LSTM layers\n",
    "            num_classes: Number of output classes (4 for bicep/tricep/supination/pronation)\n",
    "            dropout: Dropout rate\n",
    "        \"\"\"\n",
    "        super(IMU_Classifier, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=hidden_size * 2,  # *2 for bidirectional\n",
    "            num_heads=8,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Classification head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, pad_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, seq_len, input_size)\n",
    "            pad_mask: Padding mask of shape (batch_size, seq_len)\n",
    "        Returns:\n",
    "            logits: Classification logits of shape (batch_size, num_classes)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = x.size(0), x.size(1)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        lstm_out, (hidden, cell) = self.lstm(x)  # (batch_size, seq_len, hidden_size*2)\n",
    "        \n",
    "        # Apply attention mechanism\n",
    "        if pad_mask is not None:\n",
    "            # Convert pad_mask to attention mask (inverted)\n",
    "            attn_mask = pad_mask  # True for padding positions\n",
    "        else:\n",
    "            attn_mask = None\n",
    "            \n",
    "        attended_out, attention_weights = self.attention(\n",
    "            lstm_out, lstm_out, lstm_out,\n",
    "            key_padding_mask=attn_mask\n",
    "        )\n",
    "        \n",
    "        # Global average pooling over sequence dimension, ignoring padded positions\n",
    "        if pad_mask is not None:\n",
    "            # Mask out padded positions\n",
    "            mask = (~pad_mask).float().unsqueeze(-1)  # (batch_size, seq_len, 1)\n",
    "            attended_out = attended_out * mask\n",
    "            # Compute mean only over non-padded positions\n",
    "            seq_lengths = mask.sum(dim=1, keepdim=True)  # (batch_size, 1, 1)\n",
    "            pooled = attended_out.sum(dim=1) / seq_lengths.squeeze(-1)  # (batch_size, hidden_size*2)\n",
    "        else:\n",
    "            pooled = attended_out.mean(dim=1)  # (batch_size, hidden_size*2)\n",
    "        \n",
    "        # Classification\n",
    "        logits = self.classifier(pooled)  # (batch_size, num_classes)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Training function\n",
    "def train_classifier(model, train_loader, val_loader, epochs=50, lr=0.001, device='cpu'):\n",
    "    \"\"\"Training loop for the classification model\"\"\"\n",
    "    \n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_val_acc = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for batch_idx, (imu_data, labels, pad_mask) in enumerate(train_loader):\n",
    "            imu_data = imu_data.to(device)\n",
    "            labels = labels.to(device)\n",
    "            pad_mask = pad_mask.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(imu_data, pad_mask)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for imu_data, labels, pad_mask in val_loader:\n",
    "                imu_data = imu_data.to(device)\n",
    "                labels = labels.to(device)\n",
    "                pad_mask = pad_mask.to(device)\n",
    "                \n",
    "                logits = model(imu_data, pad_mask)\n",
    "                loss = criterion(logits, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(logits.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        train_acc = 100 * train_correct / train_total\n",
    "        val_acc = 100 * val_correct / val_total\n",
    "        train_loss_avg = train_loss / len(train_loader)\n",
    "        val_loss_avg = val_loss / len(val_loader)\n",
    "        \n",
    "        # Store metrics\n",
    "        train_losses.append(train_loss_avg)\n",
    "        val_losses.append(val_loss_avg)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss_avg)\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}]')\n",
    "            print(f'Train Loss: {train_loss_avg:.4f}, Train Acc: {train_acc:.2f}%')\n",
    "            print(f'Val Loss: {val_loss_avg:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "            print(f'Learning Rate: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "            print('-' * 50)\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f'Best validation accuracy: {best_val_acc:.2f}%')\n",
    "    \n",
    "    return model, {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_accuracies': val_accuracies\n",
    "    }\n",
    "\n",
    "# Updated data preparation code\n",
    "def prepare_classification_data(train_imu_segments_bicep, train_imu_segments_tricep, train_emg_segments_supination, train_emg_segments_pronation,\n",
    "                              val_imu_segments_bicep, val_imu_segments_tricep, val_imu_segments_supination, val_imu_segments_pronation):\n",
    "    \"\"\"Prepare data for classification\"\"\"\n",
    "    \n",
    "    # Combine segments and create labels\n",
    "    train_imu_segments = train_imu_segments_bicep + train_imu_segments_tricep + train_emg_segments_supination + train_emg_segments_pronation\n",
    "    train_labels = [0] * len(train_imu_segments_bicep) + [1] * len(train_imu_segments_tricep) + [2] * len(train_emg_segments_supination) + [3] * len(train_emg_segments_pronation)\n",
    "    \n",
    "    val_imu_segments = val_imu_segments_bicep + val_imu_segments_tricep + val_imu_segments_supination + val_imu_segments_pronation\n",
    "    val_labels = [0] * len(val_imu_segments_bicep) + [1] * len(val_imu_segments_tricep) + [2] * len(val_imu_segments_supination) + [3] * len(val_imu_segments_pronation)\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = IMU_Classification_Dataset(train_imu_segments, train_labels)\n",
    "    val_dataset = IMU_Classification_Dataset(val_imu_segments, val_labels)\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, \n",
    "                            collate_fn=classification_collate_fn)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, \n",
    "                          collate_fn=classification_collate_fn)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fefb7fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2]\n",
      "Train Loss: 0.2776, Train Acc: 95.54%\n",
      "Val Loss: 0.0000, Val Acc: 100.00%\n",
      "Learning Rate: 0.001000\n",
      "--------------------------------------------------\n",
      "Epoch [2/2]\n",
      "Train Loss: 0.0000, Train Acc: 100.00%\n",
      "Val Loss: 0.0000, Val Acc: 100.00%\n",
      "Learning Rate: 0.001000\n",
      "--------------------------------------------------\n",
      "Best validation accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# After your existing data loading code, add:\n",
    "train_loader, val_loader = prepare_classification_data(\n",
    "    train_imu_segments_bicep, train_imu_segments_tricep, train_imu_segments_supination, train_imu_segments_pronation,\n",
    "    val_imu_segments_bicep, val_imu_segments_tricep, val_imu_segments_supination, val_imu_segments_pronation\n",
    ")\n",
    "\n",
    "# Create and train model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = IMU_Classifier(input_size=6, hidden_size=128, num_layers=2)\n",
    "trained_model, history = train_classifier(model, train_loader, val_loader, \n",
    "                                        epochs=2, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b4d90509",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_path = './model_checkpoint/classification_model.pth'\n",
    "os.makedirs(os.path.dirname(trained_model_path), exist_ok=True)\n",
    "torch.save(trained_model.state_dict(), trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "583153b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_segment(model, imu_segment, device='cpu'):\n",
    "    \"\"\"\n",
    "    Predict the class for a single IMU segment\n",
    "    \n",
    "    Args:\n",
    "        model: Trained IMU_Classifier model\n",
    "        imu_segment: Single IMU segment as numpy array of shape (seq_len, 6) or tensor\n",
    "        device: Device to run inference on ('cpu' or 'cuda')\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains prediction, confidence, and probabilities\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Convert to tensor if numpy array\n",
    "        if isinstance(imu_segment, np.ndarray):\n",
    "            imu_tensor = torch.tensor(imu_segment, dtype=torch.float32)\n",
    "        else:\n",
    "            imu_tensor = imu_segment.clone().detach().float()\n",
    "        \n",
    "        # Add batch dimension: (1, seq_len, features)\n",
    "        imu_tensor = imu_tensor.unsqueeze(0).to(device)\n",
    "        \n",
    "        # No padding mask needed for single sequence\n",
    "        logits = model(imu_tensor, pad_mask=None)\n",
    "        \n",
    "        # Get probabilities\n",
    "        probabilities = torch.softmax(logits, dim=1)\n",
    "        \n",
    "        # Get prediction\n",
    "        predicted_class = torch.argmax(logits, dim=1).item()\n",
    "        confidence = probabilities[0, predicted_class].item()\n",
    "        \n",
    "        # Class names\n",
    "        class_names = ['bicep', 'tricep', 'supination', 'pronation']\n",
    "        predicted_label = class_names[predicted_class]\n",
    "        \n",
    "        return {\n",
    "            'prediction': predicted_class,\n",
    "            'label': predicted_label,\n",
    "            'confidence': confidence,\n",
    "            'probabilities': {\n",
    "                'bicep': probabilities[0, 0].item(),\n",
    "                'tricep': probabilities[0, 1].item(),\n",
    "                'supination' : probabilities[0, 2].item(),\n",
    "                'pronation' : probabilities[0, 3].item()\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "32c3e895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = IMU_Classifier(input_size=6, hidden_size=128, num_layers=2)\n",
    "model.load_state_dict(torch.load('./model_checkpoint/classification_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a0c2f770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: supination\n",
      "Confidence: 1.000\n",
      "Probabilities: {'bicep': 4.9547470076447e-14, 'tricep': 9.189292527097556e-18, 'supination': 1.0, 'pronation': 1.1669586466849652e-13}\n"
     ]
    }
   ],
   "source": [
    "result = predict_segment(model, train_imu_segments_supination[9], device='cpu')\n",
    "print(f\"Prediction: {result['label']}\")\n",
    "print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "print(f\"Probabilities: {result['probabilities']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EMG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
